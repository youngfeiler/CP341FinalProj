{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1afe58b6-0a95-4342-8c73-aec17964b6d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM \n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e4a03d2-d4c4-4c47-a6a7-dfe43aa02d33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"small_tokenized_data.parquet\")\n",
    "IDS = df[\"input_ids\"].tolist()\n",
    "newIDS=[]\n",
    "for each in IDS:\n",
    "    #end_index = next((i for i, x in enumerate(each) if x==0), len(each))\n",
    "    # Remove the trailing 0's\n",
    "    #each = each[:end_index]\n",
    "    # Convert the input to a tensor\n",
    "    input_tensor = torch.tensor(each).unsqueeze(0)\n",
    "    newIDS.append(each)\n",
    "    '''\n",
    "    # Feed the input tensor to the BERT model\n",
    "    outputs = model(input_tensor)\n",
    "    # Process the model output\n",
    "    logits = outputs.logits\n",
    "    probabilities = torch.softmax(logits, dim=-1)\n",
    "    #Print the predicted class probabilities\n",
    "    print(probabilities)\n",
    "    '''\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1658ea89-c839-40fa-ba9b-78a31cf211de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bert = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7178d5b4-8120-4cbd-af86-13e81e761794",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dg/jkq_f9b51f38pf3ddscgfrtm0000gn/T/ipykernel_59140/1867348514.py:2: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:248.)\n",
      "  x = torch.tensor(newIDS[:8])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 512])\n"
     ]
    }
   ],
   "source": [
    "#PyTorch tensor with first 3 reviews from our training set and pass it to it\n",
    "x = torch.tensor(newIDS[:8])\n",
    "print(x.shape)\n",
    "#this is the BERTs final layer output for each token\n",
    "y, pooled = bert(x, output_all_encoded_layers=False)\n",
    "\n",
    "#NEXT\n",
    "#nueral net model to go from pooled (vetors) to labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a3263f0-3368-4cf1-8f45-7f51f4d27731",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 768])\n",
      "torch.Size([8, 512, 768])\n"
     ]
    }
   ],
   "source": [
    "print(pooled.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af43f12b-600a-44fc-b1b0-2b9bea20232e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BertBinaryClassifier (nn.Module):\n",
    "    \n",
    "    def _init__(self, dropout=0.1):\n",
    "        super (BertBinaryClassifier, self).init()\n",
    "        self.bert = BertModel.from_pretrained ('bert-base-uncased' )\n",
    "        self.linear = nn. Linear (768, 1)\n",
    "        self.sigmoid = nn. Sigmoid ()\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        _, pooled_output = self.bert (tokens, utput_all=False)\n",
    "        l√≠near_output = self.Linear(dropout_output)\n",
    "        pred = self.sigmoid(linear_output)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "388018aa-9770-4e30-8d7a-cc9802d7d068",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nbert_clf = BertBinaryClassifier()\\nbert_clf = bert_clf.cuda()\\noptimizer = Adam(bert_clf.parameters(), lr=3e-6)\\nbert_clf.train()\\nfor epoch_num in range(EPOCHS):\\n    for step_num, batch_data in enumerate(train_dataloader):\\n        token_ids, labels = tuple(t.to(device) for t in batch_data)\\n        probas = bert_clf(token_ids)\\n        loss_func = nn.BCELoss()\\n        batch_loss = loss_func(probas, labels)\\n        bert_clf.zero_grad()\\n        batch_loss.backward()\\n        optimizer.step()\\n        \\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "bert_clf = BertBinaryClassifier()\n",
    "bert_clf = bert_clf.cuda()\n",
    "optimizer = Adam(bert_clf.parameters(), lr=3e-6)\n",
    "bert_clf.train()\n",
    "for epoch_num in range(EPOCHS):\n",
    "    for step_num, batch_data in enumerate(train_dataloader):\n",
    "        token_ids, labels = tuple(t.to(device) for t in batch_data)\n",
    "        probas = bert_clf(token_ids)\n",
    "        loss_func = nn.BCELoss()\n",
    "        batch_loss = loss_func(probas, labels)\n",
    "        bert_clf.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dd2926-3147-4a0b-9913-1c04c846dd91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
